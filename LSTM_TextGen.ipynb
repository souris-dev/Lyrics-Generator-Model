{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-TextGen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7VV2WlzkA15",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation using LSTM RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK0EP5d9EMJM",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we create an RNN (using two stacked LSTM layers) that can be used to generate song lyrics reflecting the style of a particular artist.\n",
        "\n",
        "Here, we'll be using a dataset of Tayor Swift's songs, taken from [here](https://www.kaggle.com/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums) at Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuVuP7Jjcm0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35e63346-e71a-4c2e-f1be-88efc05a24fa"
      },
      "source": [
        "# We need Tensorflow 1.x to use CuDNNLSTM properly\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GHNGzBB2i9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "586a6873-0ea9-401e-a857-f48a06a54da0"
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers import Dense, LSTM, Embedding, Input, Dropout, CuDNNLSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "\n",
        "import numpy as np\n",
        "import re # could be useful\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoJq1X6wsypa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "0174d29c-bb1d-4d58-84f2-ea268d07efcd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9PYMnSpmjED",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nugHvnMmmlct",
        "colab_type": "text"
      },
      "source": [
        "We'll be using a dataset of Tayor Swift's songs, taken from [here](https://www.kaggle.com/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums) at Kaggle.\n",
        "\n",
        "This dataset needs some preliminary preprocessing to generate a text corpus that can be easily fed into the LSTM network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t40-2BKlm7MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir('data'):\n",
        "  os.mkdir('data')\n",
        "\n",
        "if not os.path.isdir('drive/My Drive/ML & DL/Lyrics-RNN/model-checkpoints'):\n",
        "  os.mkdir('drive/My Drive/ML & DL/Lyrics-RNN/model-checkpoints')\n",
        "\n",
        "CHKP_DIR = 'drive/My Drive/ML & DL/Lyrics-RNN/model-checkpoints'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJxLZSQ3rBx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "165c66ba-b58f-453f-f042-b382e406becb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/taylor_swift_lyrics.csv', encoding='latin1')\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>album</th>\n",
              "      <th>track_title</th>\n",
              "      <th>track_n</th>\n",
              "      <th>lyric</th>\n",
              "      <th>line</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>He said the way my blue eyes shined</td>\n",
              "      <td>1</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Put those Georgia stars to shame that night</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>I said, \"That's a lie\"</td>\n",
              "      <td>3</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Just a boy in a Chevy truck</td>\n",
              "      <td>4</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>That had a tendency of gettin' stuck</td>\n",
              "      <td>5</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist         album  ... line  year\n",
              "0  Taylor Swift  Taylor Swift  ...    1  2006\n",
              "1  Taylor Swift  Taylor Swift  ...    2  2006\n",
              "2  Taylor Swift  Taylor Swift  ...    3  2006\n",
              "3  Taylor Swift  Taylor Swift  ...    4  2006\n",
              "4  Taylor Swift  Taylor Swift  ...    5  2006\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Klv2E32e7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQ_LEN = 20 # sequence length for LSTMs, i.e., act upon after recalling previous SEG_LEN words\n",
        "START_SONG = '| ' * SEQ_LEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSvwy-HCudL7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f7cad53-7342-464c-d65f-0303d9facb69"
      },
      "source": [
        "song_names = []\n",
        "lyrics = []\n",
        "\n",
        "song_number = 1 # song number in dataset\n",
        "\n",
        "first_line = True\n",
        "\n",
        "for ind, row in df.iterrows():\n",
        "\n",
        "  if (song_number == row['track_n']):\n",
        "\n",
        "    # First line of next/first song? If yes:\n",
        "    if (first_line):\n",
        "      #print('Found first line.')\n",
        "      song_names.append(row['album'])\n",
        "      lyrics.append(START_SONG + row['lyric'] + '\\n') \n",
        "      # START_SONG is the new song marker which we can use as a seed to tell the model \n",
        "      # to generate a new song from scratch\n",
        "      first_line = False\n",
        "\n",
        "      #print(lyrics)\n",
        "\n",
        "    else:\n",
        "      lyrics[len(lyrics) - 1] += row['lyric'] + '\\n' # add lyrics to this song\n",
        "\n",
        "  # Move to next song\n",
        "  else:\n",
        "    song_number = row['track_n'] # Note that song number is album-wise\n",
        "    first_line = True\n",
        "\n",
        "print(str(len(lyrics)) + ' songs processed.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94 songs processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7RGiQfc0rfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create another dataframe\n",
        "\n",
        "df_proc = pd.DataFrame({'song_name': song_names, 'lyrics': lyrics})\n",
        "\n",
        "# Save Lyrics in .txt file\n",
        "with open('data/taylor_swift_corpus.txt', 'w', encoding=\"utf-8\") as f:  \n",
        "    for lyric in lyrics:\n",
        "        f.write(lyric)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pPbg3LS1inp",
        "colab_type": "text"
      },
      "source": [
        "Yay! Now we've finally retrieved the lyrics into a more convenient format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgFUOnVpktJE",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6w9BBKwkPxu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2bf82823-16a9-4047-bb14-18e1472076f0"
      },
      "source": [
        "artist_name = 'taylor_swift'\n",
        "corpus = 'data/' + artist_name + '_corpus.txt'\n",
        "\n",
        "if not (os.path.isdir('data') and os.path.isfile(corpus)):\n",
        "  print('Data not found!')\n",
        "  raise SystemError('Corpus not found!')\n",
        "\n",
        "with open(corpus, 'r', encoding='utf-8') as f:\n",
        "  text = f.read().lower()\n",
        "\n",
        "print('Corpus length for ' + artist_name + ': ' + str(len(text)) + ' characters.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length for taylor_swift: 173961 characters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrvhJUD82MyR",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEWFt7sr7ZX_",
        "colab_type": "text"
      },
      "source": [
        "Tokenization can of course be done manually in Python, but why not make our lives easier by using what we already have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT6YV-4B2OQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keep the punctuation, but add a space before and after it\n",
        "text = re.sub('([*#@$%,?!()&*-./:[\\]^_~\\n])', r' \\1 ', text)\n",
        "# We want the line breaks too, so add a space before it and after it so that the tokenizer can recognise it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVhl8Din3-p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenization\n",
        "tokenizer = Tokenizer(char_level=False, filters='')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "token_list = tokenizer.texts_to_sequences([text])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUM90Plg5YH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.word_index\n",
        "# You can find some encoding problems like 'me\\x97' being a token apparently\n",
        "# Might use another re.sub to get rid of those"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W53wT78-5apa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DVPXSpZ7Oae",
        "colab_type": "text"
      },
      "source": [
        "## Building the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om7RPaqo7hVp",
        "colab_type": "text"
      },
      "source": [
        "Now we shall generate sequences of words based on `SEQ_LEN` for our model to work upon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70CAykxE_39h",
        "colab_type": "text"
      },
      "source": [
        "`# TODO`: Make `gen_sequences()` a generator function that yields values of X, y to avoid loading all the data into memory. The model can then be trained using `model.fit_generator()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njRZl7tC7QR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "def gen_sequences(token_list, step):\n",
        "  \"\"\"\n",
        "  step: the step of the for loop running on the token_list\n",
        "  \"\"\"\n",
        "\n",
        "  x = list()\n",
        "  y = list()\n",
        "\n",
        "  for i in range(0, len(token_list) - SEQ_LEN, step):\n",
        "    x.append(token_list[i: i + SEQ_LEN]) # the training words\n",
        "    y.append(token_list[i + SEQ_LEN]) # the word that should be predicted after them\n",
        "\n",
        "  y = to_categorical(y, num_classes=total_words)\n",
        "  # can we use keras.utils.to_categorical here? (fearing tensor vs ndarray mismatch)\n",
        "\n",
        "  num_seq = len(x)\n",
        "  print('Generated ' + str(len(x)) + ' sequences.')\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkR5K3oG9OyB",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's generate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeTHNou59Bi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "818de479-96c5-45e1-956f-90bb258cca6f"
      },
      "source": [
        "step = 1\n",
        "\n",
        "X, y = gen_sequences(token_list, step)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated 44572 sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6kN8mLE9owz",
        "colab_type": "text"
      },
      "source": [
        "## The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOVEBaKs9qqr",
        "colab_type": "text"
      },
      "source": [
        "Finally, the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n-sSIld9hDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "3a408e35-7524-41a7-cbfb-7b804c62c284"
      },
      "source": [
        "N_UNITS = 256\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "text_input = Input(shape=(None,), name='text_input')\n",
        "embd = Embedding(total_words, EMBEDDING_SIZE, name='embedding')(text_input)\n",
        "\n",
        "# CuDNNLSTM is a fast variant of LSTM, only for GPUs... might want to use that\n",
        "lstm1 = CuDNNLSTM(N_UNITS, return_sequences=True)(embd) \n",
        "lstm2 = CuDNNLSTM(N_UNITS)(lstm1)\n",
        "\n",
        "drp = Dropout(0.2)(lstm2)\n",
        "\n",
        "text_out = Dense(total_words, activation='softmax')(drp) # output text as probabilities for all words\n",
        "\n",
        "model = Model(text_input, text_out)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbZxG4yZ_c8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "dcc9143d-cc4f-4679-bf39-f0cd688d85fc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_input (InputLayer)      (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         245900    \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 256)         366592    \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 256)               526336    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2459)              631963    \n",
            "=================================================================\n",
            "Total params: 1,770,791\n",
            "Trainable params: 1,770,791\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtH0GLWQ-6HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "opt = RMSprop(lr = 0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oceNEp42rJt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some callbacks\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "callbacks = []\n",
        "mdlchkp = ModelCheckpoint(os.path.join(CHKP_DIR, 'taylor-epoch-{epoch}.h5'), period=20) # save model after every 20 epochs\n",
        "rdlr = ReduceLROnPlateau(monitor='loss', patience=3, factor=0.5, min_lr=0.0000001)\n",
        "\n",
        "callbacks = [mdlchkp, rdlr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpEykc0z_bRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1cd5ca3-1f35-4ee8-a425-752b42188bf9"
      },
      "source": [
        "model.fit(X, y , epochs=400, batch_size=32, shuffle=True, callbacks=callbacks)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "44572/44572 [==============================] - 12s 263us/step - loss: 4.4339\n",
            "Epoch 2/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 4.2090\n",
            "Epoch 3/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 3.9911\n",
            "Epoch 4/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 3.8107\n",
            "Epoch 5/400\n",
            "44572/44572 [==============================] - 12s 262us/step - loss: 3.6553\n",
            "Epoch 6/400\n",
            "44572/44572 [==============================] - 12s 263us/step - loss: 3.5479\n",
            "Epoch 7/400\n",
            "44572/44572 [==============================] - 12s 263us/step - loss: 3.4383\n",
            "Epoch 8/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 3.3726\n",
            "Epoch 9/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 3.3175\n",
            "Epoch 10/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 3.3028\n",
            "Epoch 11/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 3.3069\n",
            "Epoch 12/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 3.2637\n",
            "Epoch 13/400\n",
            "44572/44572 [==============================] - 12s 263us/step - loss: 3.2509\n",
            "Epoch 14/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 3.2452\n",
            "Epoch 15/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 3.2071\n",
            "Epoch 16/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 3.2041\n",
            "Epoch 17/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 3.2010\n",
            "Epoch 18/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 3.1795\n",
            "Epoch 19/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 3.1105\n",
            "Epoch 20/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 3.0905\n",
            "Epoch 21/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 3.0203\n",
            "Epoch 22/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.9977\n",
            "Epoch 23/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.9390\n",
            "Epoch 24/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.9293\n",
            "Epoch 25/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.9134\n",
            "Epoch 26/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.8704\n",
            "Epoch 27/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.8606\n",
            "Epoch 28/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.8409\n",
            "Epoch 29/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 2.8251\n",
            "Epoch 30/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 2.7931\n",
            "Epoch 31/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.7684\n",
            "Epoch 32/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.7345\n",
            "Epoch 33/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.7201\n",
            "Epoch 34/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.6908\n",
            "Epoch 35/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.6503\n",
            "Epoch 36/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 2.6296\n",
            "Epoch 37/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 2.6137\n",
            "Epoch 38/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.5974\n",
            "Epoch 39/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.5600\n",
            "Epoch 40/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.5400\n",
            "Epoch 41/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 2.5258\n",
            "Epoch 42/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.5065\n",
            "Epoch 43/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.4922\n",
            "Epoch 44/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.4668\n",
            "Epoch 45/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.4475\n",
            "Epoch 46/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.4245\n",
            "Epoch 47/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 2.4089\n",
            "Epoch 48/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.4032\n",
            "Epoch 49/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.3790\n",
            "Epoch 50/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.3717\n",
            "Epoch 51/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.3637\n",
            "Epoch 52/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 2.3659\n",
            "Epoch 53/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.3546\n",
            "Epoch 54/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.3488\n",
            "Epoch 55/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 2.3339\n",
            "Epoch 56/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 2.3291\n",
            "Epoch 57/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.3228\n",
            "Epoch 58/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.3028\n",
            "Epoch 59/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.3204\n",
            "Epoch 60/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.3031\n",
            "Epoch 61/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 2.3015\n",
            "Epoch 62/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 2.2959\n",
            "Epoch 63/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.2841\n",
            "Epoch 64/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.2949\n",
            "Epoch 65/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.2747\n",
            "Epoch 66/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.2815\n",
            "Epoch 67/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.2693\n",
            "Epoch 68/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 2.2943\n",
            "Epoch 69/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.2695\n",
            "Epoch 70/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.2621\n",
            "Epoch 71/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.2573\n",
            "Epoch 72/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.2594\n",
            "Epoch 73/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.2728\n",
            "Epoch 74/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 2.2629\n",
            "Epoch 75/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 2.0554\n",
            "Epoch 76/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.9222\n",
            "Epoch 77/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.8520\n",
            "Epoch 78/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.8103\n",
            "Epoch 79/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.7902\n",
            "Epoch 80/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.7561\n",
            "Epoch 81/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.7311\n",
            "Epoch 82/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.7107\n",
            "Epoch 83/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.6977\n",
            "Epoch 84/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.6783\n",
            "Epoch 85/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 1.6682\n",
            "Epoch 86/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 1.6502\n",
            "Epoch 87/400\n",
            "44572/44572 [==============================] - 12s 264us/step - loss: 1.6480\n",
            "Epoch 88/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.6295\n",
            "Epoch 89/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.6196\n",
            "Epoch 90/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.6138\n",
            "Epoch 91/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.6065\n",
            "Epoch 92/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.5987\n",
            "Epoch 93/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.5919\n",
            "Epoch 94/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.5889\n",
            "Epoch 95/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.5688\n",
            "Epoch 96/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 1.5671\n",
            "Epoch 97/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 1.5721\n",
            "Epoch 98/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.5584\n",
            "Epoch 99/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 1.5526\n",
            "Epoch 100/400\n",
            "44572/44572 [==============================] - 12s 265us/step - loss: 1.5354\n",
            "Epoch 101/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.5422\n",
            "Epoch 102/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.5391\n",
            "Epoch 103/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.5389\n",
            "Epoch 104/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.4617\n",
            "Epoch 105/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.4207\n",
            "Epoch 106/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.3912\n",
            "Epoch 107/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.3763\n",
            "Epoch 108/400\n",
            "44572/44572 [==============================] - 12s 273us/step - loss: 1.3676\n",
            "Epoch 109/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.3649\n",
            "Epoch 110/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.3484\n",
            "Epoch 111/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.3523\n",
            "Epoch 112/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.3513\n",
            "Epoch 113/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.3348\n",
            "Epoch 114/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.3300\n",
            "Epoch 115/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.3326\n",
            "Epoch 116/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.3262\n",
            "Epoch 117/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.3311\n",
            "Epoch 118/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.3109\n",
            "Epoch 119/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.3172\n",
            "Epoch 120/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.3131\n",
            "Epoch 121/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.3104\n",
            "Epoch 122/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.3162\n",
            "Epoch 123/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.3001\n",
            "Epoch 124/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2971\n",
            "Epoch 125/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.2899\n",
            "Epoch 126/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2956\n",
            "Epoch 127/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.2809\n",
            "Epoch 128/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2769\n",
            "Epoch 129/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2539\n",
            "Epoch 130/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.2704\n",
            "Epoch 131/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2784\n",
            "Epoch 132/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.2675\n",
            "Epoch 133/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.2477\n",
            "Epoch 134/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.2136\n",
            "Epoch 135/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.2154\n",
            "Epoch 136/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2053\n",
            "Epoch 137/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.2007\n",
            "Epoch 138/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2052\n",
            "Epoch 139/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1991\n",
            "Epoch 140/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1951\n",
            "Epoch 141/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1993\n",
            "Epoch 142/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2158\n",
            "Epoch 143/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1948\n",
            "Epoch 144/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.2046\n",
            "Epoch 145/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.2006\n",
            "Epoch 146/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.2011\n",
            "Epoch 147/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.2015\n",
            "Epoch 148/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1982\n",
            "Epoch 149/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1721\n",
            "Epoch 150/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1651\n",
            "Epoch 151/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1786\n",
            "Epoch 152/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1820\n",
            "Epoch 153/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1699\n",
            "Epoch 154/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1692\n",
            "Epoch 155/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1646\n",
            "Epoch 156/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1643\n",
            "Epoch 157/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1668\n",
            "Epoch 158/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1677\n",
            "Epoch 159/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1602\n",
            "Epoch 160/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1600\n",
            "Epoch 161/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1594\n",
            "Epoch 162/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1625\n",
            "Epoch 163/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1674\n",
            "Epoch 164/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1497\n",
            "Epoch 165/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1509\n",
            "Epoch 166/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1494\n",
            "Epoch 167/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1573\n",
            "Epoch 168/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1580\n",
            "Epoch 169/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1673\n",
            "Epoch 170/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1430\n",
            "Epoch 171/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1585\n",
            "Epoch 172/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1586\n",
            "Epoch 173/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1576\n",
            "Epoch 174/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1533\n",
            "Epoch 175/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1476\n",
            "Epoch 176/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1479\n",
            "Epoch 177/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.1534\n",
            "Epoch 178/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1523\n",
            "Epoch 179/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1481\n",
            "Epoch 180/400\n",
            "44572/44572 [==============================] - 12s 266us/step - loss: 1.1554\n",
            "Epoch 181/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1532\n",
            "Epoch 182/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1516\n",
            "Epoch 183/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1567\n",
            "Epoch 184/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1487\n",
            "Epoch 185/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1501\n",
            "Epoch 186/400\n",
            "44572/44572 [==============================] - 12s 273us/step - loss: 1.1634\n",
            "Epoch 187/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1454\n",
            "Epoch 188/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1552\n",
            "Epoch 189/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1512\n",
            "Epoch 190/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1522\n",
            "Epoch 191/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1445\n",
            "Epoch 192/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1437\n",
            "Epoch 193/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1520\n",
            "Epoch 194/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1547\n",
            "Epoch 195/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1492\n",
            "Epoch 196/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1382\n",
            "Epoch 197/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1401\n",
            "Epoch 198/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1566\n",
            "Epoch 199/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1468\n",
            "Epoch 200/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1417\n",
            "Epoch 201/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1505\n",
            "Epoch 202/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1450\n",
            "Epoch 203/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1385\n",
            "Epoch 204/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1465\n",
            "Epoch 205/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1528\n",
            "Epoch 206/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1425\n",
            "Epoch 207/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1388\n",
            "Epoch 208/400\n",
            "44572/44572 [==============================] - 12s 267us/step - loss: 1.1503\n",
            "Epoch 209/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1396\n",
            "Epoch 210/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1593\n",
            "Epoch 211/400\n",
            "44572/44572 [==============================] - 12s 273us/step - loss: 1.1461\n",
            "Epoch 212/400\n",
            "44572/44572 [==============================] - 12s 274us/step - loss: 1.1480\n",
            "Epoch 213/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1487\n",
            "Epoch 214/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1513\n",
            "Epoch 215/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1482\n",
            "Epoch 216/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1422\n",
            "Epoch 217/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1506\n",
            "Epoch 218/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1362\n",
            "Epoch 219/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1484\n",
            "Epoch 220/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1523\n",
            "Epoch 221/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1535\n",
            "Epoch 222/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1507\n",
            "Epoch 223/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1413\n",
            "Epoch 224/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1349\n",
            "Epoch 225/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1543\n",
            "Epoch 226/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1540\n",
            "Epoch 227/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1451\n",
            "Epoch 228/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1470\n",
            "Epoch 229/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1655\n",
            "Epoch 230/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1468\n",
            "Epoch 231/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1628\n",
            "Epoch 232/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1544\n",
            "Epoch 233/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1378\n",
            "Epoch 234/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1571\n",
            "Epoch 235/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1524\n",
            "Epoch 236/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1286\n",
            "Epoch 237/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1503\n",
            "Epoch 238/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1491\n",
            "Epoch 239/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1448\n",
            "Epoch 240/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1502\n",
            "Epoch 241/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1532\n",
            "Epoch 242/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1461\n",
            "Epoch 243/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1549\n",
            "Epoch 244/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1350\n",
            "Epoch 245/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1434\n",
            "Epoch 246/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1530\n",
            "Epoch 247/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1446\n",
            "Epoch 248/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1486\n",
            "Epoch 249/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1161\n",
            "Epoch 250/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1490\n",
            "Epoch 251/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1425\n",
            "Epoch 252/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1453\n",
            "Epoch 253/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1519\n",
            "Epoch 254/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1338\n",
            "Epoch 255/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1405\n",
            "Epoch 256/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1610\n",
            "Epoch 257/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1382\n",
            "Epoch 258/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1529\n",
            "Epoch 259/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1555\n",
            "Epoch 260/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1478\n",
            "Epoch 261/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1603\n",
            "Epoch 262/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1515\n",
            "Epoch 263/400\n",
            "44572/44572 [==============================] - 12s 273us/step - loss: 1.1415\n",
            "Epoch 264/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1472\n",
            "Epoch 265/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1452\n",
            "Epoch 266/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1334\n",
            "Epoch 267/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1503\n",
            "Epoch 268/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1530\n",
            "Epoch 269/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1463\n",
            "Epoch 270/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1518\n",
            "Epoch 271/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1539\n",
            "Epoch 272/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1449\n",
            "Epoch 273/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1497\n",
            "Epoch 274/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1541\n",
            "Epoch 275/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1425\n",
            "Epoch 276/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1456\n",
            "Epoch 277/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1461\n",
            "Epoch 278/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1464\n",
            "Epoch 279/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1401\n",
            "Epoch 280/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1430\n",
            "Epoch 281/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1421\n",
            "Epoch 282/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1491\n",
            "Epoch 283/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1540\n",
            "Epoch 284/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1567\n",
            "Epoch 285/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1449\n",
            "Epoch 286/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1446\n",
            "Epoch 287/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1400\n",
            "Epoch 288/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1483\n",
            "Epoch 289/400\n",
            "44572/44572 [==============================] - 12s 275us/step - loss: 1.1549\n",
            "Epoch 290/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1486\n",
            "Epoch 291/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1425\n",
            "Epoch 292/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1515\n",
            "Epoch 293/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1406\n",
            "Epoch 294/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1520\n",
            "Epoch 295/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1379\n",
            "Epoch 296/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1486\n",
            "Epoch 297/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1466\n",
            "Epoch 298/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1513\n",
            "Epoch 299/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1509\n",
            "Epoch 300/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1582\n",
            "Epoch 301/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1418\n",
            "Epoch 302/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1439\n",
            "Epoch 303/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1451\n",
            "Epoch 304/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1520\n",
            "Epoch 305/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1462\n",
            "Epoch 306/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1482\n",
            "Epoch 307/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1393\n",
            "Epoch 308/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1429\n",
            "Epoch 309/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1546\n",
            "Epoch 310/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1549\n",
            "Epoch 311/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1464\n",
            "Epoch 312/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1436\n",
            "Epoch 313/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1496\n",
            "Epoch 314/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1629\n",
            "Epoch 315/400\n",
            "44572/44572 [==============================] - 12s 274us/step - loss: 1.1459\n",
            "Epoch 316/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1564\n",
            "Epoch 317/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1477\n",
            "Epoch 318/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1413\n",
            "Epoch 319/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1354\n",
            "Epoch 320/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1359\n",
            "Epoch 321/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1417\n",
            "Epoch 322/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1542\n",
            "Epoch 323/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1499\n",
            "Epoch 324/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1470\n",
            "Epoch 325/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1426\n",
            "Epoch 326/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1426\n",
            "Epoch 327/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1430\n",
            "Epoch 328/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1483\n",
            "Epoch 329/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1496\n",
            "Epoch 330/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1483\n",
            "Epoch 331/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1495\n",
            "Epoch 332/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1393\n",
            "Epoch 333/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1415\n",
            "Epoch 334/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1452\n",
            "Epoch 335/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1447\n",
            "Epoch 336/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1423\n",
            "Epoch 337/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1629\n",
            "Epoch 338/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1486\n",
            "Epoch 339/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1491\n",
            "Epoch 340/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1548\n",
            "Epoch 341/400\n",
            "44572/44572 [==============================] - 12s 275us/step - loss: 1.1432\n",
            "Epoch 342/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1424\n",
            "Epoch 343/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1431\n",
            "Epoch 344/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1582\n",
            "Epoch 345/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1517\n",
            "Epoch 346/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1551\n",
            "Epoch 347/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1565\n",
            "Epoch 348/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1498\n",
            "Epoch 349/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1510\n",
            "Epoch 350/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1449\n",
            "Epoch 351/400\n",
            "44572/44572 [==============================] - 12s 268us/step - loss: 1.1506\n",
            "Epoch 352/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1412\n",
            "Epoch 353/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1477\n",
            "Epoch 354/400\n",
            "44572/44572 [==============================] - 12s 275us/step - loss: 1.1537\n",
            "Epoch 355/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1417\n",
            "Epoch 356/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1471\n",
            "Epoch 357/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1606\n",
            "Epoch 358/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1472\n",
            "Epoch 359/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1451\n",
            "Epoch 360/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1522\n",
            "Epoch 361/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1521\n",
            "Epoch 362/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1473\n",
            "Epoch 363/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1507\n",
            "Epoch 364/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1431\n",
            "Epoch 365/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1573\n",
            "Epoch 366/400\n",
            "44572/44572 [==============================] - 12s 275us/step - loss: 1.1492\n",
            "Epoch 367/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1373\n",
            "Epoch 368/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1434\n",
            "Epoch 369/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1533\n",
            "Epoch 370/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1510\n",
            "Epoch 371/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1450\n",
            "Epoch 372/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1579\n",
            "Epoch 373/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1485\n",
            "Epoch 374/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1499\n",
            "Epoch 375/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1404\n",
            "Epoch 376/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1516\n",
            "Epoch 377/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1427\n",
            "Epoch 378/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1544\n",
            "Epoch 379/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1374\n",
            "Epoch 380/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1417\n",
            "Epoch 381/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1414\n",
            "Epoch 382/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1448\n",
            "Epoch 383/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1549\n",
            "Epoch 384/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1434\n",
            "Epoch 385/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1429\n",
            "Epoch 386/400\n",
            "44572/44572 [==============================] - 12s 272us/step - loss: 1.1432\n",
            "Epoch 387/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1537\n",
            "Epoch 388/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1385\n",
            "Epoch 389/400\n",
            "44572/44572 [==============================] - 12s 273us/step - loss: 1.1270\n",
            "Epoch 390/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1514\n",
            "Epoch 391/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1464\n",
            "Epoch 392/400\n",
            "44572/44572 [==============================] - 12s 275us/step - loss: 1.1481\n",
            "Epoch 393/400\n",
            "44572/44572 [==============================] - 12s 273us/step - loss: 1.1452\n",
            "Epoch 394/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1505\n",
            "Epoch 395/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1476\n",
            "Epoch 396/400\n",
            "44572/44572 [==============================] - 12s 271us/step - loss: 1.1482\n",
            "Epoch 397/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1495\n",
            "Epoch 398/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1471\n",
            "Epoch 399/400\n",
            "44572/44572 [==============================] - 12s 270us/step - loss: 1.1498\n",
            "Epoch 400/400\n",
            "44572/44572 [==============================] - 12s 269us/step - loss: 1.1445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8da673ad68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOpSj4RstSYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('Lyrics_taylor3.h5')\n",
        "model.save_weights('Lyrics_taylor_wts3.h5')\n",
        "# aisii"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZrFlCehYPu",
        "colab_type": "text"
      },
      "source": [
        "## Generation of New Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Xa-1K2eEZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=0.2):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  preds_exp = np.exp(preds)\n",
        "\n",
        "  # The probabilities should be normalized before sampling using np.random.multinomial\n",
        "  preds = preds_exp / np.sum(preds_exp)\n",
        "\n",
        "  # Draw samples from preds, which is a multinomial distribution\n",
        "  probs = np.random.multinomial(1, preds[0], 1)\n",
        "\n",
        "  return np.argmax(probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIs07ty6vkxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_text(seed, n_next_words, model, seq_len, temperature):\n",
        "  out_text = ''\n",
        "  seed = START_SONG + seed\n",
        "\n",
        "  # We shall retrieve new words till we get n_next_words, \n",
        "  # or till the model wants to start a new song\n",
        "\n",
        "  for i in range(n_next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed])\n",
        "    #print(token_list)\n",
        "\n",
        "    # we can feed in an arbitrary number of words to the LSTM\n",
        "    # but let's stick to only the last seq_len number of words\n",
        "    # that'll help it predict faster\n",
        "    token_list = token_list[0][-seq_len:] # note that token_list was a list of lists before this line\n",
        "    \n",
        "    # Reshape it and convert to ndarray so that we an feed it in\n",
        "    token_list = np.reshape(token_list, (1, seq_len))\n",
        "\n",
        "    # Predict words!\n",
        "    probs = model.predict(token_list, verbose=0)\n",
        "    #print(np.shape(probs))\n",
        "\n",
        "    y_sampled = sample(probs, temperature)\n",
        "\n",
        "    out_word = tokenizer.index_word[y_sampled] if y_sampled > 0 else ''\n",
        "    #print(out_word)\n",
        "    \n",
        "    if out_word == '|': # the model is trying to start a new song... so finish up\n",
        "      break\n",
        "\n",
        "    if not out_word.endswith('\\n'):\n",
        "      seed += out_word + ' ' # update the seed\n",
        "      out_text += out_word + ' '\n",
        "    else:\n",
        "      seed += out_word + ' '\n",
        "      out_text += out_word\n",
        "\n",
        "  return out_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSR13Wdp4liP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('Lyrics_taylor3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9R_acsuEqAn",
        "colab_type": "text"
      },
      "source": [
        "## Let's Get Some Songs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnGv_gZa5D_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "49289b25-8b27-47bf-e518-3ee64eb29cba"
      },
      "source": [
        "outtext = gen_text('', 200, model, 20, 0.6)\n",
        "print(outtext)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i have known it all this time \n",
            "but i never thought i'd live to see it break \n",
            "it's getting dark and it's all too scar \n",
            "and i know i'm over \n",
            "but the time that you've everything back before \n",
            "you did alone , i had a but \n",
            "you're my smile , all eyes him , he's \n",
            "but i know for me , you got that sorry \n",
            "and i never saw you coming \n",
            "and i'll never be the same \n",
            "and i never saw you coming \n",
            "and i'll never be the same \n",
            "this is a together and i'll be be ? \n",
            "i never still know we had \n",
            "our hands are , all you he , i know you all \n",
            "but you're everything if just was \n",
            "and i hit you \n",
            "he ( i try to love the woods of the best of to my town \n",
            "i can't go down , no to go on the time \n",
            "you do and the dreams and oh \n",
            "but i know is we can be me \n",
            "but i know i can say about you \n",
            "and the ah \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbw_h_0iEowe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "9cc33aa7-d750-4345-d55e-46b9f9589d38"
      },
      "source": [
        "outtext = gen_text('', 500, model, 20, 0.6)\n",
        "print(outtext)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i was in your me , you got me alone \n",
            "you say that like a dream we trust too chance \n",
            "i was the was ? i was off in the white \n",
            "i don't know she know you he get me \n",
            "you have a oh things a nines \n",
            "and yeah away ( dance you live i go is a to take ? \n",
            "'cause i know i know you home \n",
            "it's so sorry \n",
            "you're so who , say \n",
            "so but they everybody saw sorry \n",
            "and , they do , we were blame to don't breathe \n",
            "i'd feeling like you , a ( wish you just right \n",
            "i had a bad feeling \n",
            "but we were dancing \n",
            "dancing with our hands tied , hands tied \n",
            "yeah , we were dancing \n",
            "like it was the first time , first time \n",
            "yeah , we were dancing \n",
            "dancing with our hands tied , hands tied \n",
            "yeah , we were dancing \n",
            "and i had a bad feeling \n",
            "but we were dancing \n",
            "i , i loved you in in of \n",
            "you'll hold me the memories \n",
            "'cause i know you were see \n",
            "that i can't get take my eyes \n",
            "long should now you'll know \n",
            "i'm beat , one eyes my mind \n",
            "but that's i know what you're love and will was \n",
            "it's i was ? \n",
            "i know what all this is me ? \n",
            "this is stood can't said you do \n",
            "all i know your little day \n",
            "this is a world and be to this door \n",
            "never moment and world , we should know , oh \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKiJ7yWiE5Lt",
        "colab_type": "text"
      },
      "source": [
        "Okay, well, of course it's not that coherent as LSTMs can't really understand the semantics of the language too well (atleast for 400 epochs). But I kind of like the last one."
      ]
    }
  ]
}